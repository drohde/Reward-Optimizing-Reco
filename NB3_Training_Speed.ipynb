{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd2976f6-fa07-413c-a593-ded6265c4dee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd2976f6-fa07-413c-a593-ded6265c4dee",
        "outputId": "656a9dbc-aa6b-454c-a092-261ba7c32514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.7/dist-packages (1.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.7.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1nMQibdolybOL1GWKB1OYxqU9KtEnrhB1\n",
            "To: /content/setup.zip\n",
            "100% 72.4M/72.4M [00:00<00:00, 144MB/s]\n",
            "Archive:  setup.zip\n",
            "replace setup/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!pip install gdown faiss-cpu\n",
        "!gdown https://drive.google.com/u/0/uc?id=1nMQibdolybOL1GWKB1OYxqU9KtEnrhB1&export=download\n",
        "!unzip setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e01124b-b8c9-45cc-9e6e-fe65f384b50c",
      "metadata": {
        "id": "4e01124b-b8c9-45cc-9e6e-fe65f384b50c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import os\n",
        "\n",
        "import scipy.sparse.linalg as sp_l\n",
        "import faiss\n",
        "import torch \n",
        "from torch import nn, optim\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "def load_tensor_spmx_dataset(path, split = 'train'):\n",
        "    load_dir = os.path.join(path, split)\n",
        "    contexts = torch.load(os.path.join(load_dir, 'contexts.pt'))\n",
        "    rewards = sparse.load_npz(os.path.join(load_dir, 'rewards.npz'))\n",
        "    return contexts, rewards\n",
        "\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "print('Loading the different splits')\n",
        "exp_folder_path = 'setup/twitch/N:100000_K:10_seed:0'\n",
        "data_folder = os.path.join(exp_folder_path, 'data')\n",
        "\n",
        "contexts_train, rewards_train = load_tensor_spmx_dataset(data_folder, split = 'train')\n",
        "contexts_val, rewards_val = load_tensor_spmx_dataset(data_folder, split = 'val')\n",
        "contexts_test, rewards_test = load_tensor_spmx_dataset(data_folder, split = 'test')\n",
        "\n",
        "print('Loading the embeddings')\n",
        "prod_emb = np.load(os.path.join(data_folder, 'prod_emb.npy'))\n",
        "P = prod_emb.shape[0]\n",
        "print('number of products is %d'%P)\n",
        "\n",
        "print('Loading the index')\n",
        "index = faiss.read_index(os.path.join(data_folder, 'index'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6df754c-bc5f-4c72-a299-215de4f418a5",
      "metadata": {
        "id": "e6df754c-bc5f-4c72-a299-215de4f418a5"
      },
      "outputs": [],
      "source": [
        "class policy_model(torch.nn.Module):\n",
        "    def __init__(self, emb):\n",
        "        super(policy_model, self).__init__()\n",
        "        self.emb = torch.Tensor(emb.T)\n",
        "        self.K, _ = self.emb.shape\n",
        "        self.theta = torch.nn.Parameter(0.005 * torch.randn(self.K, self.K))\n",
        "        self.log_sigma = torch.nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_transformed = torch.matmul(x, self.theta)\n",
        "        log_unnormalized = torch.matmul(x_transformed, self.emb)\n",
        "        return nn.functional.softmax(log_unnormalized, dim = -1), log_unnormalized\n",
        "    \n",
        "    def sample(self, x, n_samples = 1):\n",
        "        x_transformed = torch.matmul(x, self.theta)\n",
        "        log_unnormalized = torch.matmul(x_transformed, self.emb)\n",
        "        scores = torch.exp(log_unnormalized - log_unnormalized.max())\n",
        "        actions = torch.multinomial(scores, n_samples, replacement=True)\n",
        "        return actions\n",
        "    \n",
        "    def argmax(self, x):\n",
        "        x_transformed = torch.matmul(x, self.theta)\n",
        "        log_unnormalized = torch.matmul(x_transformed, self.emb)\n",
        "        return torch.argmax(log_unnormalized, dim = 1, keepdim = True)\n",
        "    \n",
        "    def Xtransformed(self, x):\n",
        "        x_transformed = torch.matmul(x, self.theta)\n",
        "        return x_transformed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41773a99-77f4-4524-bd88-98fb2a336f81",
      "metadata": {
        "tags": [],
        "id": "41773a99-77f4-4524-bd88-98fb2a336f81"
      },
      "source": [
        "### Training/Testing Routine"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf755570-3241-4d50-a679-cb10c544826f",
      "metadata": {
        "id": "bf755570-3241-4d50-a679-cb10c544826f"
      },
      "source": [
        "We define the code to test out the policies once trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95abc358-88c1-432f-9b4e-3168eed3b2d8",
      "metadata": {
        "id": "95abc358-88c1-432f-9b4e-3168eed3b2d8"
      },
      "outputs": [],
      "source": [
        "def test_policy(policy, index, contexts, rewards, bsize):\n",
        "    \n",
        "    greed_reward_index, greed_reward = 0., 0.\n",
        "    N_test = contexts.size(0)\n",
        "    idxlist = np.arange(N_test)\n",
        "    \n",
        "    for bnum, st_idx in tqdm(enumerate(range(0, N_test, bsize))):\n",
        "            end_idx = min(st_idx + bsize, N_test)\n",
        "            indices = idxlist[st_idx:end_idx]\n",
        "            len_indices = len(indices)\n",
        "            help_broadcast = np.arange(len_indices)\n",
        "            \n",
        "            r = rewards[indices]\n",
        "            c = contexts[indices]\n",
        "            \n",
        "            _, top_actions_index = index.search(policy.Xtransformed(c).cpu().detach().numpy(), k = 1)\n",
        "            top_rewards_index = rewards[indices[:,np.newaxis], top_actions_index].A\n",
        "            \n",
        "            top_actions = policy.argmax(c).cpu().numpy()\n",
        "            top_rewards = rewards[indices].A[help_broadcast[:,np.newaxis], top_actions]\n",
        "            \n",
        "            \n",
        "            r_a_index = top_rewards_index.sum()\n",
        "            r_a = top_rewards.sum()\n",
        "\n",
        "            greed_reward_index += r_a_index/N_test\n",
        "            greed_reward += r_a/N_test\n",
        "                \n",
        "    return greed_reward_index, greed_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d803048c-66fe-4d1b-b0cd-37302521cf84",
      "metadata": {
        "id": "d803048c-66fe-4d1b-b0cd-37302521cf84"
      },
      "source": [
        "We also define the training routine boiler-plate that every algorithm will use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2314f64b-e4e6-4faf-86bf-6bd8da1ab67e",
      "metadata": {
        "id": "2314f64b-e4e6-4faf-86bf-6bd8da1ab67e"
      },
      "outputs": [],
      "source": [
        "def training_routine(algorithm_loss, pi, index, n_samples, P, trunc_at, eps, contexts, rewards, epochs, bsize, lr, reg, val_contexts, val_rewards) :\n",
        "    \n",
        "    optimizer = torch.optim.Adam(pi.parameters(), lr=lr, weight_decay=reg)\n",
        "    N_train = contexts.size(0)\n",
        "    idxlist = np.arange(N_train)\n",
        "    proposal_probs = torch.ones(bsize, P) * eps/P\n",
        "    \n",
        "    train_m_rewards = []\n",
        "    val_max, val_max_index = [0.]*epochs, [0.]*epochs\n",
        "    \n",
        "    true_duration = 0.\n",
        "    \n",
        "    for i in range(epochs):\n",
        "        print(\"epoch number %d\"%i)\n",
        "        np.random.shuffle(idxlist)\n",
        "        # train for one epoch\n",
        "        for bnum, st_idx in tqdm(enumerate(range(0, N_train, bsize))):\n",
        "            base_time = time.time()\n",
        "            \n",
        "            end_idx = min(st_idx + bsize, N_train)\n",
        "            indices = idxlist[st_idx:end_idx]\n",
        "            indices_len = len(indices)\n",
        "            \n",
        "            X = contexts[indices]\n",
        "\n",
        "            loss = algorithm_loss(pi, contexts, rewards, indices, index, n_samples, proposal_probs)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            true_duration += time.time() - base_time\n",
        "            \n",
        "            _, top_actions = index.search(pi.Xtransformed(X).detach().numpy(), k = 1)\n",
        "            top_rewards = rewards[indices[:,np.newaxis], top_actions].A\n",
        "            r_a = top_rewards.mean()\n",
        "            train_m_rewards.append(r_a)\n",
        "            \n",
        "        print('Computing metrics on val')\n",
        "        max_perf_index, max_perf = test_policy(pi, index, val_contexts, val_rewards, 512)\n",
        "        val_max_index[i], val_max[i] = max_perf_index, max_perf\n",
        "        print('Argmax reward on the validation : INDEX %.4f, TRUE %.4f'%(max_perf_index, max_perf))\n",
        "        \n",
        "    return train_m_rewards, val_max_index, val_max, true_duration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e12b2ef9-da7a-4bf7-84ac-bdd953e7f485",
      "metadata": {
        "id": "e12b2ef9-da7a-4bf7-84ac-bdd953e7f485"
      },
      "source": [
        "## Reinforce"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4e808c8-0b8c-47c2-8f20-67c24b4d2bd6",
      "metadata": {
        "id": "c4e808c8-0b8c-47c2-8f20-67c24b4d2bd6"
      },
      "source": [
        "In large scale recommendation problems, we usually deal with a considerable amount of observations making stochastic gradient descent and its variants suitable for such application. For a single observation $x_i$, we are interested in the gradient of $\\hat{R}_i(\\pi_\\theta) = \\mathbf{\\mathbb{E}}_{a \\sim \\pi_\\theta(.|x_i)}[\\hat{r}(a, x_i)]$ which can be derived using the log trick giving the following formula:\n",
        "\n",
        "\\begin{align}\n",
        "\\label{reinforce}\n",
        "\\nabla_\\theta \\hat{R}_i(\\pi_\\theta) = \\mathbf{\\mathbb{E}}_{a \\sim \\pi_\\theta(.|x_i)}[\\hat{r}(a, x_i)\\nabla_\\theta \\log \\pi_\\theta(a|x_i)].\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41fdd430-0107-43d7-94de-fef30b0c67e1",
      "metadata": {
        "id": "41fdd430-0107-43d7-94de-fef30b0c67e1"
      },
      "outputs": [],
      "source": [
        "def reinforce_loss(pi, contexts, rewards, indices, index, n_samples, proposal_probs):\n",
        "    \n",
        "    X = contexts[indices]\n",
        "    indices_len = len(indices)\n",
        "    probs, _ = pi(X)\n",
        "    with torch.no_grad():\n",
        "        actions = torch.multinomial(probs, n_samples, replacement=True)\n",
        "        r_a = torch.Tensor(rewards[indices[:,np.newaxis], actions].A)\n",
        "\n",
        "    help_broadcast = np.arange(indices_len)[:,np.newaxis]\n",
        "    return -torch.mean(r_a * torch.log(probs[help_broadcast, actions]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4d269e8-b1f8-47f4-9232-eacfb0e1d0c7",
      "metadata": {
        "id": "a4d269e8-b1f8-47f4-9232-eacfb0e1d0c7"
      },
      "source": [
        "This gradient can be computed exactly as an expectation over a discrete distribution, our current policy $\\pi_\\theta$, when the action space is of small size. Once the size of the catalog $P$ is in the order of millions, an exact gradient update becomes a bottleneck for the optimization process because of the complexity of the following computations:\n",
        "\n",
        "**1 - Computing $\\nabla_\\theta \\log \\pi_\\theta(.|x_i)$**: \n",
        "\n",
        "We need to deal with the normalizing constant $Z_\\theta(x_i)$ present in the computation of $\\nabla_\\theta \\log \\pi_\\theta(.|x_i)$. Indeed, $Z_\\theta(x_i)$ is a sum over all the action space and its computation needs to be avoided if we hope to reduce the complexity of the gradient update.\n",
        "\n",
        "**2 - Computing the expectation**:\n",
        "\n",
        "The expectation is a sum over all the action space and is obviously computed in $\\mathcal{O}(P)$. To avoid this expensive sum, we can resort to sampling from $\\pi_\\theta$ to approximate the gradient. This allows us to obtain the Reinforce estimator, an unbiased estimator of the expectation but does not change the complexity of the method which stays linear in the catalog size. Indeed, sampling needs the computation of $Z_\\theta(x_i)$ or can be done with the gumbel trick \\cite{gumbel} which both scale in $\\mathcal{O}(P)$. To lower the time complexity, we need to avoid sampling directly from $\\pi_\\theta$ and use Monte Carlo techniques instead such as importance sampling/rejection sampling with carefully chosen proposals to achieve fast sampling and accurate gradient approximation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e94506ac-8026-4057-8d58-94af8909d524",
      "metadata": {
        "id": "e94506ac-8026-4057-8d58-94af8909d524"
      },
      "source": [
        "# Accelerating Reinforce"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbd010ac-ed80-46a0-9852-012bc8984f25",
      "metadata": {
        "id": "fbd010ac-ed80-46a0-9852-012bc8984f25"
      },
      "source": [
        "As pointed out in the previous section, we need a workaround to deal with the presence of the normalizing constant in the gradient. For a fixed observations $x_i$, we can push further the computation of $\\nabla_\\theta \\log \\pi_\\theta(a|x_i)$ to obtain a quantity that does not involve $Z_\\theta(x_i)$. Indeed, we have for a fixed action $a$:\n",
        "\n",
        "\\begin{align*}\n",
        "\\nabla_\\theta \\log \\pi_\\theta(a|x_i) &= \\nabla_\\theta f_\\theta(a, x_i) - \\nabla_\\theta \\log Z_\\theta(x_i) \\\\\n",
        "&= \\nabla_\\theta f_\\theta(a, x_i) - \\frac{\\nabla_\\theta Z_\\theta(x_i)}{Z_\\theta(x_i)} \\\\\n",
        "&= \\nabla_\\theta f_\\theta(a, x_i) - \\sum_b \\pi_\\theta(b|x_i) \\nabla_\\theta f_\\theta(b, x_i) \\\\\n",
        "&= \\nabla_\\theta f_\\theta(a, x_i) -  \\mathbf{\\mathbb{E}}_{b \\sim \\pi_\\theta(.|x_i)}[\\nabla_\\theta f_\\theta(b, x_i)]\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "Injecting the above expression of $\\nabla_\\theta \\log \\pi_\\theta(a|x_i)$ into the reinforce gradient leads us to the following covariance gradient:\n",
        "\n",
        "\\begin{align}\n",
        "\\nabla_\\theta \\hat{R}_i(\\pi_\\theta) = \\mathbf{Cov}_{a \\sim \\pi_\\theta(.|x_i)}[\\hat{r}(a, x_i),  \\nabla_\\theta f_\\theta(a, x_i)] \\label{cov_grad}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "with \n",
        "\\begin{align*}\n",
        "    \\mathbf{Cov}[A, \\boldsymbol{B}] &= \\mathbf{\\mathbb{E}}[A.\\boldsymbol{B}] - \\mathbf{\\mathbb{E}}[A].\\mathbf{\\mathbb{E}}[\\boldsymbol{B}] \\\\\n",
        "    &= \\mathbf{\\mathbb{E}}[(A - \\mathbf{\\mathbb{E}}[A] ).(\\boldsymbol{B} - \\mathbf{\\mathbb{E}}[\\boldsymbol{B}])] \n",
        "\\end{align*}\n",
        "\n",
        "Now that we got rid of the normalizing constant inside the gradient, we need to approximate the expectation with Self Normalized Importance Sampling:\n",
        "\n",
        "$$\\mathbf{\\mathbb{E}}_{\\pi_\\theta(.|x_i)}[g(a)] \\approx \\sum_{s = 1}^S \\bar{\\omega}_s g(a_s)$$\n",
        " \n",
        "\n",
        "with $a_s \\sim q \\quad \\forall s$, $\\omega_s = \\frac{\\exp \\{f(a_s, x_i)\\}}{q(a_s)}$ and $\\bar{\\omega}_s = \\frac{\\omega_s}{\\sum_{s' = 1}^S \\omega_{s'}}$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04d06a93-1479-4fea-884a-d98f9b6ade2e",
      "metadata": {
        "id": "04d06a93-1479-4fea-884a-d98f9b6ade2e"
      },
      "source": [
        "### SNIPS - Uniform Proposal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ac506c8-09de-445a-a4be-a24f184aebc8",
      "metadata": {
        "id": "9ac506c8-09de-445a-a4be-a24f184aebc8"
      },
      "source": [
        "we use the uniform proposal to approximate the gradient :\n",
        "\n",
        "$$q(a|x) = \\frac{1}{P}$$ with P the catalg size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ca6aa17-c00e-4c43-b174-5cdda677d23e",
      "metadata": {
        "id": "3ca6aa17-c00e-4c43-b174-5cdda677d23e"
      },
      "outputs": [],
      "source": [
        "def uniform_loss(pi, contexts, rewards, indices, index, n_samples, proposal_probs):\n",
        "    \n",
        "    X = contexts[indices]\n",
        "    len_indices = len(indices)\n",
        "    x_transformed = torch.matmul(X, pi.theta)\n",
        "    a_samples = torch.randint(P, [len_indices, n_samples])\n",
        "    a_embs = pi.emb[:, a_samples]\n",
        "\n",
        "    log_p_tilde = torch.einsum('ij,jik->ik', x_transformed, a_embs)\n",
        "    with torch.no_grad():\n",
        "        ws = torch.nn.functional.softmax(log_p_tilde, dim=-1) # SNIPS\n",
        "\n",
        "    r_a = torch.Tensor(rewards[indices[:,np.newaxis], a_samples].A)\n",
        "\n",
        "    mean_log_p_tilde, mean_rewards = torch.sum(ws * log_p_tilde, dim=-1, keepdim=True), torch.sum(ws * r_a, dim=-1, keepdim=True)\n",
        "    \n",
        "    return - torch.mean(torch.sum(ws * (log_p_tilde - mean_log_p_tilde) * (r_a - mean_rewards), dim=-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90b307f4-4eb2-4ede-8ecb-57bcd977e8cc",
      "metadata": {
        "id": "90b307f4-4eb2-4ede-8ecb-57bcd977e8cc"
      },
      "source": [
        "## SNIPS - Mixture Proposal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48a59782-a4fd-4fb4-a306-d29813109620",
      "metadata": {
        "id": "48a59782-a4fd-4fb4-a306-d29813109620"
      },
      "source": [
        "We use a proposal that uses the MIPS index in training time as well, which is a mixture between a uniform (to cover the first training phase) and the head of the distribution (covers the late training phases when the policy concentrates on the good items).\n",
        "\n",
        "By constructing the head of the distribution (K top actions, with K a hyperparameter) $\\alpha_K(x_i)= {\\rm argsort}(h_{\\theta}(x_i)^T\\beta)_{1:K}$ with the help of MIPS, we define :\n",
        "\n",
        "$$\n",
        "    q_{K, \\epsilon}(a|x_i)= \n",
        "\\begin{cases}\n",
        "    \\frac{\\epsilon}{P} + (1-\\epsilon)\\kappa(a|x_i),              & \\text{if } a \\in \\alpha_K(x_i) \\\\\n",
        "    \\frac{\\epsilon}{P},& \\text{otherwise }.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Where $\\epsilon$ is a parameter that controls the mixture and \n",
        "$$\n",
        "\\kappa(a|x_i) = \\frac{\\exp(h_{\\theta}(x_i)^T\\beta_a)}{\\sum_{a' \\in \\alpha_K(x_i)}  \\exp(h_{\\theta}(x_i)^T\\beta_{a'})}1[a \\in \\alpha_K(x_i)].\n",
        "$$\n",
        "\n",
        "Note that $\\epsilon = 1.$ retrieves the uniform proposal. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "101ab6ff-d74c-48cf-8929-1e8b30775b22",
      "metadata": {
        "id": "101ab6ff-d74c-48cf-8929-1e8b30775b22"
      },
      "outputs": [],
      "source": [
        "def mixture_loss(pi, contexts, rewards, indices, index, n_samples,proposal_probs):\n",
        "    \n",
        "    len_indices = len(indices)\n",
        "    help_broadcast = np.arange(len_indices)[:,np.newaxis]\n",
        "\n",
        "    X = contexts[indices]\n",
        "    x_transformed = torch.matmul(X, pi.theta)\n",
        "\n",
        "    query = x_transformed.cpu().detach().numpy()\n",
        "    topK_scores, topK_indices = index.search(query, k = trunc_at)\n",
        "\n",
        "\n",
        "    topK_indices, topK_scores = torch.tensor(topK_indices), torch.tensor(topK_scores)\n",
        "    topK_probs = torch.nn.functional.softmax(topK_scores, dim=-1)\n",
        "\n",
        "    proposal_probs[help_broadcast, topK_indices] += (1. - eps) * topK_probs\n",
        "\n",
        "    uni_n_samples = int(n_samples * eps)\n",
        "    a_samples_uni = torch.randint(P, [len_indices, uni_n_samples])\n",
        "    a_samples_topK = topK_indices[help_broadcast, torch.multinomial(topK_probs, n_samples - uni_n_samples, replacement=True)]\n",
        "\n",
        "    a_samples = torch.cat([a_samples_uni, a_samples_topK], dim=1)\n",
        "\n",
        "    a_embs = pi.emb[:, a_samples]\n",
        "    log_p_tilde = torch.einsum('ij,jik->ik', x_transformed, a_embs)\n",
        "\n",
        "    # SNIPS\n",
        "    with torch.no_grad():\n",
        "        log_ws_tilde = log_p_tilde - torch.log(proposal_probs[help_broadcast, a_samples])\n",
        "        ws = torch.nn.functional.softmax(log_ws_tilde, dim=-1)\n",
        "\n",
        "    r_a = torch.Tensor(rewards[indices[:,np.newaxis], a_samples].A)\n",
        "    mean_log_p_tilde, mean_rewards = torch.sum(ws * log_p_tilde, dim=-1, keepdim=True), torch.sum(ws * r_a, dim=-1, keepdim=True)\n",
        "    \n",
        "    proposal_probs[help_broadcast, topK_indices] = eps/P\n",
        "    \n",
        "    return - torch.mean(torch.sum(ws * (log_p_tilde - mean_log_p_tilde) * (r_a - mean_rewards), dim=-1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7a6c657-c2fe-40d5-8684-d5e6c66d23a9",
      "metadata": {
        "id": "e7a6c657-c2fe-40d5-8684-d5e6c66d23a9"
      },
      "source": [
        "## Experiments :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba9bd2ac-9450-4052-89f4-f659d3533555",
      "metadata": {
        "id": "ba9bd2ac-9450-4052-89f4-f659d3533555"
      },
      "outputs": [],
      "source": [
        "epochs = 1\n",
        "bsize = 32\n",
        "reg = 1e-10\n",
        "\n",
        "lr = 1e-4\n",
        "n_samples = 1000\n",
        "eps = 0.8\n",
        "trunc_at = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2019e2bc-29b2-45e1-b7ef-f46a6411da33",
      "metadata": {
        "id": "2019e2bc-29b2-45e1-b7ef-f46a6411da33"
      },
      "source": [
        "### Training the policies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df346fce-36a4-4090-bc88-f155032cb361",
      "metadata": {
        "id": "df346fce-36a4-4090-bc88-f155032cb361"
      },
      "outputs": [],
      "source": [
        "# Reinforce\n",
        "policy_reinforce = policy_model(prod_emb)\n",
        "train_r, val_r, _, duration_r = training_routine(reinforce_loss, policy_reinforce, index, n_samples, P, trunc_at, eps, \n",
        "                                                 contexts_train, rewards_train, epochs, bsize, lr, reg, contexts_val, rewards_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b48183a-a871-4ada-85fe-ee354727d32c",
      "metadata": {
        "id": "8b48183a-a871-4ada-85fe-ee354727d32c"
      },
      "outputs": [],
      "source": [
        "# Uniform proposal\n",
        "policy_uniform = policy_model(prod_emb)\n",
        "train_u, val_u, _, duration_u = training_routine(uniform_loss, policy_uniform, index, n_samples, P, trunc_at, eps, \n",
        "                                                              contexts_train, rewards_train, epochs, bsize, lr, reg, contexts_val, rewards_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d01ace50-fd71-4959-a9a5-2e48a7f83e6a",
      "metadata": {
        "id": "d01ace50-fd71-4959-a9a5-2e48a7f83e6a"
      },
      "outputs": [],
      "source": [
        "# Mixture proposal\n",
        "policy_mixture = policy_model(prod_emb)\n",
        "train_m, _, val_m, duration_m = training_routine(mixture_loss, policy_mixture, index, n_samples, P, trunc_at, eps, \n",
        "                                                 contexts_train, rewards_train, epochs, bsize, lr, reg, contexts_val, rewards_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f4864bc-c8a0-4346-9dd7-ca0b5cc72e61",
      "metadata": {
        "id": "9f4864bc-c8a0-4346-9dd7-ca0b5cc72e61"
      },
      "source": [
        "### Relative speed-up :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b67bc2c-7192-4239-8bcd-fcf56dd8245c",
      "metadata": {
        "id": "3b67bc2c-7192-4239-8bcd-fcf56dd8245c"
      },
      "outputs": [],
      "source": [
        "x = ['Reinforce', 'Mixture', 'Uniform']\n",
        "y = [1., duration_r/duration_m, duration_r/duration_u]\n",
        "\n",
        "sns.barplot(x = x, y = y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e7d0943-56bb-4e5e-8355-ac5d6fa3f01d",
      "metadata": {
        "id": "0e7d0943-56bb-4e5e-8355-ac5d6fa3f01d"
      },
      "source": [
        "## Performance:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare the performance of all the methods, one needs to run extensive experiments on larger portions of the datasets for longer epochs. This can't be achieved in a tutorial so we invite the most curious to dive into the paper : [Fast Offline Policy Optimization for Large Scale Recommendation](https://arxiv.org/abs/2208.05327)"
      ],
      "metadata": {
        "id": "B5yJ1-OuIhL3"
      },
      "id": "B5yJ1-OuIhL3"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ES2kWZ7CJSto"
      },
      "id": "ES2kWZ7CJSto",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "NB3 - Training Speed.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}